# 如何使用 AI 在本项目基础上进行规范化开发

本文档说明如何借助 AI 助手（如 GitHub Copilot、Claude、ChatGPT 等）在 py_ref 项目基础上进行高效、规范的开发。

## 📋 目录

- [项目概述](#项目概述)
- [AI 辅助开发最佳实践](#ai-辅助开发最佳实践)
- [常用 AI 提示词模板](#常用-ai-提示词模板)
- [开发工作流](#开发工作流)
- [代码质量保证](#代码质量保证)
- [常见场景示例](#常见场景示例)

## 项目概述

py_ref 是一个标准的 Python 项目参照模板，已配置：
- ✅ 规范的项目结构（src layout）
- ✅ 完整的代码质量工具（black, isort, ruff）
- ✅ 完善的测试框架（pytest）
- ✅ Rich 日志系统
- ✅ 自动化 CI/CD
- ✅ 中文注释和文档

## AI 辅助开发最佳实践

### 1. 提供充分的上下文

与 AI 交互时，提供以下信息可以获得更好的结果：

```
我正在基于 py_ref 项目开发，这是一个标准的 Python 项目模板。
项目特点：
- 使用 src layout 结构
- 所有代码、注释和日志使用中文
- 使用 black + isort + ruff 进行代码质量控制
- 使用 pytest 进行测试，目标覆盖率 >90%
- 使用 rich 进行终端输出美化
- 使用 uv 进行包管理

当前需求：[描述你的需求]
```

### 2. 明确代码规范要求

在请求 AI 生成代码时，明确说明：

```
请按照以下规范生成代码：
1. 所有注释使用中文
2. 所有日志消息使用中文
3. 函数和类使用英文命名（遵循 PEP 8）
4. 文档字符串使用中文（Google 风格）
5. 每个函数必须有类型注解
6. 必须包含对应的单元测试
7. 使用 py_ref.logger 进行日志记录
```

### 3. 请求完整的解决方案

不要只要代码片段，要求 AI 提供：

```
请提供完整的解决方案，包括：
1. 源代码文件（src/py_ref/目录下）
2. 单元测试文件（tests/目录下）
3. 使用示例
4. 需要添加的依赖（如果有）
```

## 常用 AI 提示词模板

### 模板 1：创建新功能模块

```
我需要在 py_ref 项目中添加一个新的 [功能名称] 模块。

要求：
1. 在 src/py_ref/ 下创建 [module_name].py
2. 实现以下功能：
   - [功能1描述]
   - [功能2描述]
3. 使用中文注释和文档字符串
4. 使用 py_ref.logger 记录日志（中文消息）
5. 在 tests/ 下创建 test_[module_name].py
6. 测试覆盖率要达到 95% 以上
7. 在 src/py_ref/__init__.py 中导出公共 API
8. 符合项目现有的代码风格

请提供完整代码。
```

### 模板 2：修复 Bug

```
我在 [文件名] 中遇到以下问题：
[问题描述]

错误信息：
[错误堆栈]

请帮我：
1. 分析问题原因
2. 提供修复方案
3. 更新相关测试
4. 确保所有测试通过
5. 保持代码符合 black/isort/ruff 规范
```

### 模板 3：优化现有代码

```
请帮我优化 [文件名] 中的 [函数/类名]。

当前代码：
[粘贴代码]

优化目标：
- [性能/可读性/可维护性]

要求：
1. 保持功能不变
2. 保持 API 兼容性
3. 更新相关测试
4. 添加中文注释说明优化点
5. 符合项目代码规范
```

### 模板 4：编写测试

```
请为 [模块/函数] 编写全面的单元测试。

被测代码位于：src/py_ref/[module].py

测试要求：
1. 使用 pytest 框架
2. 测试文件：tests/test_[module].py
3. 包含以下测试场景：
   - 正常情况测试
   - 边界条件测试
   - 异常情况测试
   - 参数化测试（pytest.mark.parametrize）
4. 使用中文编写测试文档字符串
5. 使用 fixture 进行测试数据准备
6. 目标覆盖率 95%+
```

### 模板 5：添加新依赖

```
我需要在项目中使用 [包名] 来实现 [功能]。

请帮我：
1. 更新 pyproject.toml 添加依赖
2. 提供使用示例代码
3. 说明如何安装（使用 uv）
4. 如果需要，更新相关文档
5. 添加相应的测试
```

### 模板 6：重构代码

```
请帮我重构 [文件名]，使其更加模块化和可维护。

当前问题：
- [问题1]
- [问题2]

重构目标：
- 提高代码可读性
- 降低复杂度
- 保持向后兼容

要求：
1. 保持所有现有测试通过
2. 添加新测试覆盖重构的代码
3. 使用中文注释说明设计决策
4. 符合 SOLID 原则
5. 符合项目代码规范
```

## 开发工作流

### 步骤 1：需求分析

与 AI 对话明确需求：

```
我想实现 [功能描述]。

请帮我：
1. 分析这个功能应该包含哪些模块
2. 设计合理的 API
3. 说明需要哪些依赖
4. 评估对现有代码的影响
5. 建议项目结构调整（如果需要）
```

### 步骤 2：代码实现

使用具体的提示词请求代码：

```
基于上述设计，请实现：

1. 创建 src/py_ref/[module].py
   - 使用中文注释
   - 完整的类型注解
   - 使用 py_ref.logger 记录日志
   - 异常处理要完善

2. 创建 tests/test_[module].py
   - 全面的测试用例
   - 使用 fixture
   - 中文测试文档

3. 更新 src/py_ref/__init__.py
   - 导出新的公共 API

请提供完整代码。
```

### 步骤 3：代码审查

请求 AI 审查代码：

```
请审查以下代码：

[粘贴代码]

检查项：
1. 是否符合 PEP 8
2. 是否符合项目代码规范
3. 中文注释是否清晰
4. 类型注解是否完整
5. 异常处理是否合理
6. 是否有安全问题
7. 性能是否可优化
8. 测试覆盖是否充分

请给出具体的改进建议。
```

### 步骤 4：测试验证

```
我实现了 [功能]，请帮我：

1. 设计全面的测试场景
2. 生成测试用例
3. 包含边界测试和异常测试
4. 确保测试覆盖率 >95%
```

### 步骤 5：文档更新

```
请帮我更新以下文档：

1. README.md 添加新功能说明
2. docs/CHANGELOG.md 添加变更记录
3. 在代码中添加使用示例
4. 如果需要，更新 docs/QUICKSTART.md

要求所有文档使用中文。
```

## 代码质量保证

### 使用 AI 进行代码检查

```
请检查以下代码是否符合项目规范：

[粘贴代码]

检查清单：
- [ ] 通过 ruff check
- [ ] 通过 black --check
- [ ] 通过 isort --check
- [ ] 所有注释使用中文
- [ ] 所有日志使用中文
- [ ] 类型注解完整
- [ ] 文档字符串完整
- [ ] 有对应的测试
- [ ] 测试覆盖率 >90%

请指出不符合的地方并给出修改建议。
```

### 请求 AI 生成测试

```
请为以下函数生成完整的测试：

[粘贴函数代码]

测试要求：
1. 测试正常情况
2. 测试所有边界条件
3. 测试异常情况
4. 使用 @pytest.mark.parametrize 进行参数化测试
5. 测试文档使用中文
6. 如果需要，使用 mock
7. 确保代码覆盖率 100%
```

### Git 提交规范

遵循简洁明了的提交规范，每次提交应该只做一件事。使用中文描述，格式：`类型: 简短说明`。类型包括：feat（新功能）、fix（修复）、docs（文档）、refactor（重构）、test（测试）、chore（构建/工具）。

### Linus Torvalds 开发理念

**"Talk is cheap, show me the code"** - 代码胜于雄辩，用简洁高效的代码说话。**"Bad programmers worry about the code. Good programmers worry about data structures"** - 专注于设计好的数据结构和算法。**保持简单（KISS）** - 避免过度设计，优先选择最简单可行的方案。

### API 设计规范

使用 FastAPI 开发 RESTful API 时遵循以下规范：

1. **统一响应格式**：所有 API 返回 `{"code": 0, "data": {}, "message": "成功"}`
2. **RESTful 风格**：GET 查询、POST 创建、PUT 更新、DELETE 删除
3. **版本控制**：路由使用 `/api/v1/` 前缀
4. **错误处理**：使用 HTTPException，自定义错误码
5. **文档完整**：每个端点添加中文描述和示例
6. **类型验证**：使用 Pydantic 模型进行请求/响应验证
7. **日志记录**：记录所有请求和异常
8. **安全性**：添加认证、限流、CORS 配置

参考 `src/py_ref/api.py` 了解标准实现。

## 常见场景示例

### 场景 1：添加数据库模块

```
我需要在 py_ref 项目中添加数据库操作模块。

要求：
1. 支持 SQLite 和 PostgreSQL
2. 使用 SQLAlchemy ORM
3. 实现连接池管理
4. 完善的异常处理
5. 使用中文日志记录所有操作
6. 提供便捷的查询接口
7. 包含数据库迁移支持

请提供：
- src/py_ref/database.py 完整代码
- tests/test_database.py 测试代码
- pyproject.toml 依赖更新
- 使用示例
```

### 场景 2：添加 Web API 模块

```
我需要使用 FastAPI 创建 REST API。

要求：
1. 在 src/py_ref/api.py 中实现
2. 支持异步操作
3. 使用 pydantic 进行数据验证
4. 集成 py_ref.logger 进行请求日志记录（中文）
5. 完整的错误处理和响应
6. API 文档自动生成
7. 包含认证中间件

请提供完整实现和测试。
```

### 场景 3：添加配置管理

```
我需要添加配置文件管理功能。

要求：
1. 支持 YAML 和 TOML 格式
2. 支持环境变量覆盖
3. 类型安全的配置访问
4. 配置验证
5. 开发/生产环境配置分离
6. 使用中文注释和日志

请提供：
- src/py_ref/config.py
- tests/test_config.py
- 示例配置文件
- 使用文档
```

### 场景 4：添加命令行界面

```
我需要为项目添加 CLI。

要求：
1. 使用 click 或 typer
2. 支持子命令
3. 丰富的帮助信息（中文）
4. 使用 rich 美化输出
5. 进度条显示
6. 错误处理和用户友好的错误信息
7. 支持配置文件

请实现完整的 CLI 模块。
```

## AI 工具推荐

### 1. GitHub Copilot
- **优势**：IDE 集成，实时代码补全
- **使用场景**：编写代码时的自动补全和建议

### 2. ChatGPT / Claude
- **优势**：深度对话，架构设计，代码审查
- **使用场景**：需求分析、设计讨论、代码重构

### 3. Cursor
- **优势**：AI 驱动的编辑器，理解项目上下文
- **使用场景**：大规模重构，多文件修改

## 提示词技巧

### 1. 提供项目上下文

将项目的关键信息告诉 AI：
```
我的项目特点：
- 项目名：py_ref
- 结构：src layout
- Python 版本：3.13
- 包管理：uv
- 代码质量：black + isort + ruff
- 测试框架：pytest
- 日志系统：rich
- 语言：所有注释和日志使用中文
```

### 2. 使用清单式需求

```
请实现 XX 功能，需要：
- [ ] 功能点 1
- [ ] 功能点 2
- [ ] 功能点 3
- [ ] 完整测试
- [ ] 中文文档
```

### 3. 请求多个方案

```
请提供 3 种实现方案，分别是：
1. 最简单的实现（快速原型）
2. 生产级实现（完整功能）
3. 高性能实现（优化版本）

每个方案包含：代码、测试、优缺点分析
```

### 4. 增量式开发

```
第一步：请设计基本结构和接口
第二步：实现核心功能
第三步：添加错误处理
第四步：编写测试
第五步：优化和文档

让我们从第一步开始。
```

## 常见问题

### Q1: AI 生成的代码不符合项目规范怎么办？

**A**: 在提示词中明确规范，并提供示例：

```
请参考以下代码风格生成新代码：

[粘贴项目中符合规范的代码示例]

要求新代码：
1. 保持相同的代码风格
2. 使用相同的日志方式
3. 使用相同的注释风格
4. 使用相同的测试模式
```

### Q2: 如何让 AI 理解项目特定的架构？

**A**: 提供架构文档和示例：

```
我的项目架构：

src/py_ref/
├── core.py      # 核心功能
├── logger.py    # 日志模块
└── main.py      # 入口

所有模块都：
- 在 __init__.py 导出公共 API
- 使用 logger 记录操作
- 有对应的测试文件

请按照这个架构添加新模块 [module_name]。
```

### Q3: 如何确保生成的测试充分？

**A**: 提供测试清单：

```
请确保测试包含：
- [ ] 正常流程测试（happy path）
- [ ] 边界值测试
- [ ] 异常情况测试
- [ ] 并发安全测试（如适用）
- [ ] 性能测试（如适用）
- [ ] 集成测试
- [ ] 使用 mock 隔离依赖
- [ ] 覆盖率 >95%
```

## 最佳实践总结

1. **明确上下文**：让 AI 理解你的项目结构和规范
2. **提供示例**：展示期望的代码风格
3. **分步实现**：不要一次要求太多
4. **持续迭代**：根据 AI 反馈不断改进提示词
5. **代码审查**：始终人工审查 AI 生成的代码
6. **运行测试**：确保所有测试通过
7. **遵循规范**：使用项目的质量检查工具验证

## 参考资源

- [项目快速开始](QUICKSTART.md)
- [发布指南](RELEASE.md)
- [变更日志](CHANGELOG.md)
- [项目总结](PROJECT_SUMMARY.md)

---

**使用 AI 辅助开发，让编程更高效、更规范！** 🚀
